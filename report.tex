

Abstract.

Introduction.



LDA Model
1. Generative Model
Latent Dirichlet allocation treats each document in a corpus of text as a "bag of words", generated
by a probabilistic model.
As with any model, LDA makes simplifiying assumptions so that useful information can be extracted
from the corpus.
Each document is a random combination of topics.
Each topic is a random combination of words from some fixed vocabulary.
Each word in each document is randomly chosen from the combination of topics sampled for the document.
The number of words in each document is also randomly chosen.
All of these random values and combinations are sampled according to particular distributions with
known parameters.

insert generative model here


2. Gibbs Sampling
While during the generative process the parameters are known, given the resultant corpus alone, it is unclear
which parameters were used.
However, using the given corpus, the parameters of the original model can be inferred or estimated.







Creative
1. PCA
The word embedding provides some semantic and syntactic information about each word in a very high
dimensional space.
Any particular corpus, especially the ones with which we are working, contains a very small subset of
of the words which the word embedding describes.
We conjectured that perhaps the variance of the words in the corpus within the embedding space
could be described with a more concise representation than the full embedding dimensionality.
We decided to apply Principal Component Analysis (PCA) to the subset of the word embedding which describes
the input corpus.
We used sklearn's PCA implementation, which allows the user to specify the size of the reduced
dimensionality.
Since we are working in a very high dimensional space and there can be much variability between
many corpora, we decided to dynamically choose the size of the reduced embedding space to capture
at least 97\% of the variance in the given corpus.

Bibiography
